# jax2onnx

🚧 **Disclaimer / Development Status**

**Warning:** *This project is currently under active and heavy development.*

This repository (**jax2onnx v0.2.0**) represents a fundamental shift from the initial approach. It now leverages JAX’s own intermediate representation (**jaxpr**) as the core of the ONNX export strategy, greatly simplifying and enhancing robustness and maintainability.

Special thanks and credit go to community members from discussions [Feature Request #4430 on flax forum](https://github.com/google/flax/issues/4430) and [Feature Request #26430 on jax-ml forum](https://github.com/jax-ml/jax/issues/26430), especially [@limarta](https://github.com/limarta), whose elegant [jaxpr-to-ONNX demonstration](https://gist.github.com/limarta/855a88cc1c0163487a9dc369891147ab) significantly influenced the new approach.

Key changes and benefits:

- 🌟 **JAXPR-based**: Uses JAX’s built-in jaxpr as the foundation for a clean, maintainable conversion process.
- 🧩 **Flexible Plugin Architecture:** Retains the extensibility of the original jax2onnx design, enabling straightforward integration of low-level JAX primitives and higher-level modules (like Flax nnx).

Rather than waiting for everything to be complete, the current implementation is shared early to encourage collaboration and community contributions. Feedback, issues, and pull requests are highly appreciated!



![img.png](https://enpasos.github.io/jax2onnx/images/img1.png)

`jax2onnx` converts your JAX/Flax model directly to ONNX format.  

### **how ...**

To save any model or function to ONNX format, use the `save_onnx` function:

```py
from jax2onnx import save_onnx

my_module = CoolTransformerModel(...)  # your JAX module or function
save_onnx(
    callable=my_module,
    input_shapes=[('B', 30)],   # input shapes with batch dimension 'B'
    model_file_name="my_module.onnx"
)
```



### **Approach**
Components can be easily added as plugins, including their test cases, which are automatically picked up by **pytest**. Each test case sends random input tensors through the JAX/Flax model and compares the output with the ONNX model to ensure correctness.

This library follows a **test-driven and demand-driven** approach, giving you **full control** over how JAX/Flax components are mapped to ONNX—**no hidden magic, no black-box abstraction**. While it may not cover every use case out of the box, you can **extend it by adding your own plugins** and contribute them back to the project. 🚀

### **Supported JAX/ONNX Components (representating the state of v0.1.0)**


 
<!-- AUTOGENERATED TABLE START -->

| JAX Component | ONNX Components | Testcases | Since |
|:-------------|:---------------|:---------|:------|
| [flax.nnx.BatchNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.BatchNorm) | [BatchNormalization](https://onnx.ai/onnx_0_1/operators/onnx__BatchNormalization.html) | `batchnorm` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/batchnorm_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/batchnorm_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/batchnorm_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/batchnorm_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.Conv](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv) | [Conv](https://onnx.ai/onnx_0_1/operators/onnx__Conv.html) | `conv_3x3_1` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_1_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_1_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_1_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_1_11.onnx "dynamic batch dim + more shape info")<br>`conv_3x3_2` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_2_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_2_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_2_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_2_11.onnx "dynamic batch dim + more shape info")<br>`conv_3x3_3` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_3_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_3_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_3_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/conv_3x3_3_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.Dropout](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/stochastic.html#flax.nnx.Dropout) | [Dropout](https://onnx.ai/onnx_0_1/operators/onnx__Dropout.html) | `dropout` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_11.onnx "dynamic batch dim + more shape info")<br>`dropout_1d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_1d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_1d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_1d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_1d_11.onnx "dynamic batch dim + more shape info")<br>`dropout_2d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_2d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_2d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_2d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_2d_11.onnx "dynamic batch dim + more shape info")<br>`dropout_3d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_3d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_3d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_3d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_3d_11.onnx "dynamic batch dim + more shape info")<br>`dropout_4d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_4d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_4d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_4d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_4d_11.onnx "dynamic batch dim + more shape info")<br>`dropout_high` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_high_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_high_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_high_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_high_11.onnx "dynamic batch dim + more shape info")<br>`dropout_low` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_low_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_low_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_low_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dropout_low_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.LayerNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm) | [LayerNormalization](https://onnx.ai/onnx_0_1/operators/onnx__LayerNormalization.html) | `layernorm_default` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_default_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_default_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_default_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_default_11.onnx "dynamic batch dim + more shape info")<br>`layernorm_multiaxis` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_multiaxis_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_multiaxis_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_multiaxis_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/layernorm_multiaxis_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) | [Gemm](https://onnx.ai/onnx_0_1/operators/onnx__Gemm.html)<br>[MatMul](https://onnx.ai/onnx_0_1/operators/onnx__MatMul.html) | `linear` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_11.onnx "dynamic batch dim + more shape info")<br>`linear_2d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_2d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_2d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_2d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_2d_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.LinearGeneral](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.LinearGeneral) | [Gemm](https://onnx.ai/onnx_0_1/operators/onnx__Gemm.html)<br>[MatMul](https://onnx.ai/onnx_0_1/operators/onnx__MatMul.html) | `linear_general` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_11.onnx "dynamic batch dim + more shape info")<br>`linear_general_2` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_2_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_2_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_2_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_2_11.onnx "dynamic batch dim + more shape info")<br>`linear_general_mha_projection` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection_11.onnx "dynamic batch dim + more shape info")<br>`linear_general_mha_projection2` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection2_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection2_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection2_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/linear_general_mha_projection2_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.MultiHeadAttention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.MultiHeadAttention) | [Reshape](https://onnx.ai/onnx_0_1/operators/onnx__Reshape.html)<br>[Gemm](https://onnx.ai/onnx_0_1/operators/onnx__Gemm.html)<br>[Einsum](https://onnx.ai/onnx_0_1/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx_0_1/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx_0_1/operators/onnx__Softmax.html) | `multihead_attention` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/multihead_attention_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/multihead_attention_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/multihead_attention_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/multihead_attention_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.avg_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.avg_pool) | [AveragePool](https://onnx.ai/onnx_0_1/operators/onnx__AveragePool.html) | `avg_pool` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/avg_pool_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/avg_pool_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/avg_pool_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/avg_pool_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.dot_product_attention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.dot_product_attention) | [Constant](https://onnx.ai/onnx_0_1/operators/onnx__Constant.html)<br>[Einsum](https://onnx.ai/onnx_0_1/operators/onnx__Einsum.html)<br>[Mul](https://onnx.ai/onnx_0_1/operators/onnx__Mul.html)<br>[Softmax](https://onnx.ai/onnx_0_1/operators/onnx__Softmax.html) | `dot_product_attention` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_11.onnx "dynamic batch dim + more shape info")<br>`dot_product_attention_shape_check` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_shape_check_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_shape_check_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_shape_check_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_shape_check_11.onnx "dynamic batch dim + more shape info")<br>`dot_product_attention_softmax_axis` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_softmax_axis_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_softmax_axis_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_softmax_axis_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/dot_product_attention_softmax_axis_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.elu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.elu) | [Elu](https://onnx.ai/onnx_0_1/operators/onnx__Elu.html) | `elu` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/elu_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/elu_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/elu_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/elu_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.gelu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.gelu) | [Gelu](https://onnx.ai/onnx_0_1/operators/onnx__Gelu.html) | `gelu` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu_11.onnx "dynamic batch dim + more shape info")<br>`gelu2` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu2_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu2_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu2_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu2_11.onnx "dynamic batch dim + more shape info")<br>`gelu3` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu3_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu3_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu3_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gelu3_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.leaky_relu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.leaky_relu) | [LeakyRelu](https://onnx.ai/onnx_0_1/operators/onnx__LeakyRelu.html) | `leaky_relu` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/leaky_relu_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/leaky_relu_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/leaky_relu_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/leaky_relu_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.log_softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.log_softmax) | [LogSoftmax](https://onnx.ai/onnx_0_1/operators/onnx__LogSoftmax.html) | `log_softmax` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/log_softmax_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/log_softmax_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/log_softmax_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/log_softmax_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.max_pool](https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.max_pool) | [MaxPool](https://onnx.ai/onnx_0_1/operators/onnx__MaxPool.html) | `max_pool` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/max_pool_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/max_pool_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/max_pool_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/max_pool_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.relu](https://docs.jax.dev/en/latest/_autosummary/jax.nn.relu.html#jax.nn.relu) | [Relu](https://onnx.ai/onnx_0_1/operators/onnx__Relu.html) | `relu` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/relu_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/relu_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/relu_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/relu_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.sigmoid](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.sigmoid) | [Sigmoid](https://onnx.ai/onnx_0_1/operators/onnx__Sigmoid.html) | `sigmoid` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/sigmoid_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/sigmoid_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/sigmoid_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/sigmoid_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.softmax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softmax) | [Softmax](https://onnx.ai/onnx_0_1/operators/onnx__Softmax.html) | `softmax` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softmax_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softmax_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softmax_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softmax_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.softplus](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.softplus) | [Softplus](https://onnx.ai/onnx_0_1/operators/onnx__Softplus.html) | `softplus` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softplus_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softplus_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softplus_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/softplus_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [flax.nnx.tanh](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.tanh) | [Tanh](https://onnx.ai/onnx_0_1/operators/onnx__Tanh.html) | `tanh` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tanh_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tanh_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tanh_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tanh_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.lax.broadcast](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.broadcast.html) | [Expand](https://onnx.ai/onnx_0_1/operators/onnx__Expand.html) | `broadcast_a` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/broadcast_a_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/broadcast_a_10.onnx "static batch dim + more shape info")<br>`broadcast_b` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/broadcast_b_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/broadcast_b_10.onnx "static batch dim + more shape info") | v0.1.0 |
| [jax.lax.gather](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.gather.html) | [Gather](https://onnx.ai/onnx_0_1/operators/onnx__Gather.html) | `gather_tf_out` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gather_tf_out_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gather_tf_out_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gather_tf_out_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/gather_tf_out_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.lax.slice](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.slice.html) | [Slice](https://onnx.ai/onnx_0_1/operators/onnx__Slice.html) | `slice_basic` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_basic_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_basic_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_basic_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_basic_11.onnx "dynamic batch dim + more shape info")<br>`slice_last_column` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_last_column_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_last_column_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_last_column_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_last_column_11.onnx "dynamic batch dim + more shape info")<br>`slice_out_of_bounds` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_out_of_bounds_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_out_of_bounds_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_out_of_bounds_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_out_of_bounds_11.onnx "dynamic batch dim + more shape info")<br>`slice_single_element` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_single_element_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_single_element_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_single_element_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_single_element_11.onnx "dynamic batch dim + more shape info")<br>`slice_with_stride` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_with_stride_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_with_stride_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_with_stride_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/slice_with_stride_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.add](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.add.html) | [Add](https://onnx.ai/onnx_0_1/operators/onnx__Add.html) | `add` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/add_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/add_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/add_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/add_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.concat](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.concat.html) | [Concat](https://onnx.ai/onnx_0_1/operators/onnx__Concat.html) | `concat` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/concat_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/concat_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/concat_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/concat_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.einsum](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html) | [Einsum](https://onnx.ai/onnx_0_1/operators/onnx__Einsum.html) | `einsum_attention` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_attention_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_attention_10.onnx "static batch dim + more shape info")<br>`einsum_batch_matmul` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_batch_matmul_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_batch_matmul_10.onnx "static batch dim + more shape info")<br>`einsum_dynamic_batch_matmul` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_dynamic_batch_matmul_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_dynamic_batch_matmul_10.onnx "static batch dim + more shape info")<br>`einsum_dynamic_batch_matmul_batched` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_dynamic_batch_matmul_batched_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_dynamic_batch_matmul_batched_10.onnx "static batch dim + more shape info")<br>`einsum_matmul` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_matmul_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_matmul_10.onnx "static batch dim + more shape info")<br>`einsum_matmul_static` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_matmul_static_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/einsum_matmul_static_10.onnx "static batch dim + more shape info") | v0.1.0 |
| [jax.numpy.matmul](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.matmul.html) | [MatMul](https://onnx.ai/onnx_0_1/operators/onnx__MatMul.html) | `matmul_2d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_2d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_2d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_2d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_2d_11.onnx "dynamic batch dim + more shape info")<br>`matmul_3d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_3d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_3d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_3d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_3d_11.onnx "dynamic batch dim + more shape info")<br>`matmul_4d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_4d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_4d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_4d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/matmul_4d_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.reshape](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.reshape.html) | [Reshape](https://onnx.ai/onnx_0_1/operators/onnx__Reshape.html) | `reshapeA` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeA_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeA_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeA_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeA_11.onnx "dynamic batch dim + more shape info")<br>`reshapeB` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeB_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeB_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeB_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeB_11.onnx "dynamic batch dim + more shape info")<br>`reshapeC` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeC_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeC_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeC_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeC_11.onnx "dynamic batch dim + more shape info")<br>`reshapeD` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeD_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeD_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeD_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeD_11.onnx "dynamic batch dim + more shape info")<br>`reshapeE` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeE_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeE_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeE_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/reshapeE_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.squeeze](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.squeeze.html) | [Squeeze](https://onnx.ai/onnx_0_1/operators/onnx__Squeeze.html) | `squeeze_dynamic_batch` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_dynamic_batch_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_dynamic_batch_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_dynamic_batch_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_dynamic_batch_11.onnx "dynamic batch dim + more shape info")<br>`squeeze_multiple_dims` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_multiple_dims_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_multiple_dims_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_multiple_dims_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_multiple_dims_11.onnx "dynamic batch dim + more shape info")<br>`squeeze_single_dim` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_single_dim_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_single_dim_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_single_dim_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_single_dim_11.onnx "dynamic batch dim + more shape info")<br>`squeeze_vit_output` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_vit_output_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_vit_output_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_vit_output_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/squeeze_vit_output_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.tile](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html) | [Tile](https://onnx.ai/onnx_0_1/operators/onnx__Tile.html) | `tile_a` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_a_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_a_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_a_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_a_11.onnx "dynamic batch dim + more shape info")<br>`tile_b` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_b_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_b_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_b_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_b_11.onnx "dynamic batch dim + more shape info")<br>`tile_c` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_c_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_c_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_c_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/tile_c_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| [jax.numpy.transpose](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html) | [Transpose](https://onnx.ai/onnx_0_1/operators/onnx__Transpose.html) | `transpose_4d` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_4d_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_4d_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_4d_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_4d_11.onnx "dynamic batch dim + more shape info")<br>`transpose_basic` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_basic_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_basic_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_basic_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_basic_11.onnx "dynamic batch dim + more shape info")<br>`transpose_high_dim` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_high_dim_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_high_dim_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_high_dim_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_high_dim_11.onnx "dynamic batch dim + more shape info")<br>`transpose_reverse` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_reverse_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_reverse_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_reverse_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_reverse_11.onnx "dynamic batch dim + more shape info")<br>`transpose_square_matrix` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_square_matrix_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_square_matrix_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_square_matrix_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transpose_square_matrix_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |

<!-- AUTOGENERATED TABLE END -->

✅ = passed<br>
❌ = failed

Examples

<!-- AUTOGENERATED EXAMPLES TABLE START -->

| Component | Description | Children | Testcases | Since |
|:----------|:------------|:---------|:---------|:------|
| CNN | A MNIST CNN model with convolutional and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax | `mnist_cnn_1` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_1_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_1_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_1_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_1_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| CNN | A MNIST CNN model with convolutional, layer norm, pooling, dropout, and linear layers. | flax.nnx.Conv<br>flax.nnx.Linear<br>flax.nnx.relu<br>flax.nnx.avg_pool<br>flax.nnx.reshape<br>flax.nnx.log_softmax<br>flax.nnx.LayerNorm<br>flax.nnx.Dropout<br>flax.nnx.max_pool | `mnist_cnn_2` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_2_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_2_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_2_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_cnn_2_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| ConvEmbedding | Convolutional Token Embedding for MNIST with hierarchical downsampling. | flax.nnx.Conv<br>flax.nnx.LayerNorm<br>jax.numpy.Reshape<br>jax.nn.relu | `mnist_conv_embedding` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_conv_embedding_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_conv_embedding_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_conv_embedding_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_conv_embedding_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| MLP | A simple Multi-Layer Perceptron (MLP) with BatchNorm, Dropout, and GELU activation. | flax.nnx.Linear<br>flax.nnx.BatchNorm<br>flax.nnx.Dropout<br>flax.nnx.gelu | `mlp` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| MLP Block | MLP in Transformer | flax.nnx.Linear<br>flax.nnx.Dropout<br>flax.nnx.gelu | `mlp_block` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_block_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_block_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_block_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mlp_block_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| PatchEmbedding | Cutting the image into patches and linearly embedding them. | flax.nnx.Linear<br>jax.numpy.Transpose<br>jax.numpy.Reshape | `patch_embedding` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/patch_embedding_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/patch_embedding_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/patch_embedding_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/patch_embedding_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| TransformerBlock | Transformer from 'Attention Is All You Need.' | flax.nnx.MultiHeadAttention<br>flax.nnx.LayerNorm<br>MLPBlock<br>flax.nnx.Dropout | `transformer_block` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transformer_block_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transformer_block_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transformer_block_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/transformer_block_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |
| ViT | A MNIST Vision Transformer (ViT) model | ConvEmbedding<br>PatchEmbedding<br>TransformerBlock<br>flax.nnx.Linear<br>flax.nnx.LayerNorm<br>jax.lax.gather | `mnist_vit_conv` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_conv_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_conv_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_conv_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_conv_11.onnx "dynamic batch dim + more shape info")<br>`mnist_vit_patch` [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_patch_00.onnx "static batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_patch_10.onnx "static batch dim + more shape info") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_patch_01.onnx "dynamic batch dim") [✅](https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx_0_1/mnist_vit_patch_11.onnx "dynamic batch dim + more shape info") | v0.1.0 |

<!-- AUTOGENERATED EXAMPLES TABLE END -->


Versions of Major Dependencies

| Library       | jax2onnx v0.2.0 | 
|:--------------|:----------------| 
| `JAX`         | v0.5.2          | 
| `Flax`        | v0.10.4         | 
| `onnx`        | v1.17.0         |  
| `onnxruntime` | v1.20.1         |  

Note: for more details look into the `pyproject.toml` file

### **Limitations**
The onnx graph is composed in memory and then saved to disk. This may lead to memory issues for large models.



### **Supported Configurations**
 
 
<img src="https://enpasos.github.io/jax2onnx/images/variants.png" alt="variants" width="400">

| `internal shape info` | `dynamic batch dim` | Behavior |
|-----------------------|---------------------|----------|
| **True**              | **False**           | Default setting: Internal shape information is included, batch size is fixed. |
| **False**             | **False**           | Like the default, but without internal shape annotations. |
| **False**             | **True**            | Shape information is only provided for input and output tensors, with batch dimensions set dynamically (`'B'`). |
| **True**              | **True**            | Shape information is included on all connections where possible, with batch dimensions set dynamically (`'B'`). However, in cases where the batch dimension is merged with other dimensions internally, shape annotations must be omitted. |






### **How to Contribute**

If you'd like to see a specific model or function supported, consider contributing by adding a plugin for an existing module or function under the `jax2onnx/plugins` directory. You can also contribute by adding an example to the `examples` directory.

Of course, any other improvements are welcome as well!

### **Installation**

To install `jax2onnx` ... **t.b.d. - meanwhile use latest development version**:

```bash
pip install jax2onnx  
```

or to install the latest development version:

```bash
pip install -i https://test.pypi.org/simple/ jax2onnx
```
 

### **Pre and Post Transpose**

One of the challenges when converting models from JAX/Flax to ONNX is handling differences in tensor dimension conventions. For example, many JAX/Flax components (such as convolution layers) work with inputs in **NHWC** format (Batch, Height, Width, Channels), whereas ONNX’s convolution operator expects inputs in **NCHW** format (Batch, Channels, Height, Width).

To bridge this gap, `jax2onnx` supports two optional parameters when calling `to_onnx` on generating the ONNX model:

- **`pre_transpose`**:  
  A list of tuples that specify how to rearrange the dimensions of the **input tensor** before the conversion. For example, passing `[(0, 3, 1, 2)]` will convert a JAX/Flax input in NHWC format to ONNX’s NCHW format.  
 
- **`post_transpose`**:  
  A list of tuples that specify how to rearrange the dimensions of the **output tensor** after the ONNX conversion. This parameter is useful if you need to convert the output back from ONNX’s format to the expected JAX/Flax format (or any other preferred layout). For example, using `[(0, 2, 3, 1)]` will convert an output in NCHW format back to NHWC.  
 
In the **Conv** plugin, for instance, the input shape is first converted from ONNX (NCHW) to JAX (NHWC) using the helper function `onnx_shape_to_jax_shape`.

 
### **Dirty Details**
Mapping components between JAX and ONNX can be inherently challenging due to fundamental differences in how they represent computations. In JAX, a function and its parameters are evaluated dynamically at runtime, whereas in ONNX, the computational graph is static and defined at conversion time (via JAX2ONNX) before being used at runtime.

Rather than attempting to match all possible runtime parameters of a JAX component to an ONNX equivalent, we focus on mapping only those relevant to a specific use case. This tradeoff between simplicity and generality ensures practicality but requires awareness of the intended use case. There are two points in time when the use case can be known:
1. **During Testing** – In unit tests, we explicitly define the parameters passed to the `to_onnx` function, which is monkey-patched onto the component. To ensure the correct mapping of JAX function parameters to their ONNX counterparts, `to_onnx` must return a function that accepts the JAX function parameters. An example of this approach can be found in `jax2onnx/plugins/jax/lax/slice.py`.
2. **During Usage** – Ideally, our `to_onnx` implementation allows users to use JAX components as they normally would. However, in some cases, users may need to wrap a component in one of our `PartialWithOnnx` or `Supports2Onnx` classes. These wrappers enable fine-tuning of `to_onnx` behavior in conjunction with the component’s JAX `__call__` function. For an example, see `ReshapeWithOnnx(Supports2Onnx)` in `tests/examples/mnist_cnn.py`.

This approach ensures flexibility while maintaining compatibility between JAX and ONNX, balancing ease of use with the necessary constraints of a static computational graph.



### **License**
This project is licensed under the terms of the Apache License, Version 2.0. See the `LICENSE` file for details.

  

