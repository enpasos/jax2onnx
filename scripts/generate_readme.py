# scripts/generate_readme.py

import json
import logging
import subprocess
import time
from pathlib import Path
from typing import Any

from tests.t_generator import (
    get_plugin_grouping,
)

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(message)s")

# Paths
BASE_DIR = Path(__file__).parent
COVERAGE_PATH = BASE_DIR / "../docs/readme/coverage_tables.md"
REPORT_PATH = BASE_DIR / "output/pytest_report.json"
REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)  # Ensure output dir exists

# Markers for auto-generated sections in docs/readme/coverage_tables.md
START_MARKER = "<!-- AUTOGENERATED TABLE START -->"
END_MARKER = "<!-- AUTOGENERATED TABLE END -->"
EXAMPLES_START_MARKER = "<!-- AUTOGENERATED EXAMPLES TABLE START -->"
EXAMPLES_END_MARKER = "<!-- AUTOGENERATED EXAMPLES TABLE END -->"

# old: NETRON_BASE_URL = "https://netron.app/?url=https://enpasos.github.io/jax2onnx/onnx/"
NETRON_BASE_URL = "https://netron.app/?url=https://huggingface.co/enpasos/jax2onnx-models/resolve/main/"


# --- Running Tests ---
def run_pytest() -> dict[tuple[str, str, str], str]:
    """Runs pytest and captures test results."""
    logging.info("üõ† Running tests...")

    try:
        subprocess.run(
            ["pytest", "--json-report", f"--json-report-file={REPORT_PATH}"],
            capture_output=True,
            text=True,
            check=False,
        )
    except subprocess.CalledProcessError as e:
        logging.error(f"‚ùå Pytest execution failed: {e}")
        return {}

    return parse_pytest_results()


def parse_pytest_results() -> dict[tuple[str, str, str], str]:
    """Parses pytest JSON report and extracts test results."""
    if not REPORT_PATH.exists():
        logging.warning("‚ö†Ô∏è No pytest report found.")
        return {}

    with REPORT_PATH.open("r", encoding="utf-8") as f:
        data = json.load(f)

    test_results = {}
    for test in data.get("tests", []):
        parts = test["nodeid"].split("::")

        if len(parts) == 3:
            partA, partB, partC = parts
        elif len(parts) == 2:
            partA, partC = parts
            partB = "NoClass"
        else:
            logging.warning(f"‚ö†Ô∏è Unexpected nodeid format: {test['nodeid']}")
            continue

        partAs = partA.split("/")
        if len(partAs) < 2 or partAs[1] not in {"examples", "primitives"}:
            continue

        testcase_name = (
            partC.replace("test_", "") if partC.startswith("test_") else partC
        )
        situation = partAs[1]  # "examples" or "primitives"
        context = partAs[-1]
        context = (
            context.replace("test_", "") if context.startswith("test_") else context
        )
        context = context.replace(".py", "") if context.endswith(".py") else context
        context = situation + "." + context
        plugin = partB.replace("Test_", "") if partB.startswith("Test_") else partB

        status = "‚úÖ" if test["outcome"] == "passed" else "‚ùå"
        key = (context, plugin, testcase_name)
        test_results[key] = status

    logging.info(f"‚úÖ {len(test_results)} test results captured.")
    return test_results


# --- Metadata Processing ---
def merge_test_results(
    grouped: dict[tuple[str, str], list[dict[str, Any]]], test_results: dict
) -> dict[tuple[str, str], list[dict[str, Any]]]:
    """Ensures all metadata appears in tables, even if no test result exists."""
    for (context, component), testcases in grouped.items():
        for tc in testcases:
            tc_name = tc["testcase"]
            result = test_results.get(
                (context, component, tc_name), "‚ûñ"
            )  # Always set a result
            url = f"{NETRON_BASE_URL}{context.replace('.', '/')}/{tc_name}.onnx"
            tc["result"] = f"[`{tc_name}`]({url}) {result}"

    logging.info(
        f"üìå Merged {len(grouped)} grouped metadata entries with test results."
    )
    return grouped


# --- Coverage doc Update ---
def update_coverage_tables(
    grouped_metadata: dict[tuple[str, str], list[dict[str, Any]]], test_results: dict
):
    """Updates docs/readme/coverage_tables.md with new plugin and example tables."""
    plugins_grouped = {
        k: v for k, v in grouped_metadata.items() if k[0].startswith("primitives")
    }
    examples_grouped = {
        k: v for k, v in grouped_metadata.items() if k[0].startswith("examples")
    }

    plugins_grouped = merge_test_results(plugins_grouped, test_results)
    examples_grouped = merge_test_results(examples_grouped, test_results)

    new_plugins_section = generate_markdown_table(plugins_grouped, is_example=False)
    new_examples_section = generate_markdown_table(examples_grouped, is_example=True)

    coverage_content = COVERAGE_PATH.read_text(encoding="utf-8")

    coverage_content = replace_markers(
        coverage_content, START_MARKER, END_MARKER, new_plugins_section
    )
    coverage_content = replace_markers(
        coverage_content,
        EXAMPLES_START_MARKER,
        EXAMPLES_END_MARKER,
        new_examples_section,
    )

    COVERAGE_PATH.write_text(coverage_content, encoding="utf-8")
    logging.info("‚úÖ docs/readme/coverage_tables.md updated successfully!")


def generate_markdown_table(
    grouped: dict[tuple[str, str], list[dict[str, Any]]], is_example: bool
) -> str:
    """Generates markdown table for docs/readme/coverage_tables.md, ensuring all metadata is included."""
    if is_example:
        header = (
            "| Component | Description | Testcases | Since |\n"
            "|:----------|:------------|:----------|:------|"
        )
        rows = []
        for (context, component), data in sorted(grouped.items()):
            description = data[0].get("description", "‚ûñ")
            # children = data[0].get("children", [])
            # children_str = "<br>".join(children) if children else "‚ûñ"
            testcases_str = "<br>".join(tc["result"] for tc in data) if data else "‚ûñ"
            since = data[0].get("since", "‚ûñ")  # Ensure "since" is included
            row = f"| {component} | {description} | {testcases_str} | {since} |"
            rows.append(row)

    else:
        header = (
            "| JAX Component | ONNX Components | Testcases | Since |\n"
            "|:-------------|:---------------|:---------|:------|"
        )
        rows = []
        for (context, component), data in sorted(grouped.items()):
            # Remove the leading "primitives." prefix from the context
            display_context = context.removeprefix("primitives.")

            comp_name = component.removeprefix("plugins.")
            # Add the context (like "jnp") as a prefix to the component name
            prefixed_comp_name = f"{display_context}.{comp_name}"
            jax_comp = f"[{prefixed_comp_name}]({data[0]['jax_doc']})"
            onnx_components = (
                "<br>".join(
                    sorted([f"[{x['component']}]({x['doc']})" for x in data[0]["onnx"]])
                )
                if data[0]["onnx"]
                else "‚ûñ"
            )
            testcases_str = "<br>".join(tc["result"] for tc in data) if data else "‚ûñ"
            since = data[0].get("since", "‚ûñ")
            row = f"| {jax_comp} | {onnx_components} | {testcases_str} | {since} |"
            rows.append(row)

    logging.info(f"üìã Generated {len(rows)} table rows.")
    return f"{header}\n" + "\n".join(rows)


def replace_markers(
    content: str, start_marker: str, end_marker: str, new_section: str
) -> str:
    """Replaces section between markers in docs/readme/coverage_tables.md, ensuring no data loss."""
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    if start_idx == -1 or end_idx == -1:
        logging.error(
            f"‚ùå Markers `{start_marker}` or `{end_marker}` not found in docs/readme/coverage_tables.md"
        )
        return content

    return (
        content[:start_idx]
        + f"{start_marker}\n\n{new_section}\n\n{end_marker}"
        + content[end_idx + len(end_marker) :]
    )


# --- Main Execution ---
if __name__ == "__main__":
    start_time = time.time()
    test_results = run_pytest()
    grouped_metadata = get_plugin_grouping()
    update_coverage_tables(grouped_metadata, test_results)
    logging.info(f"‚è≥ Total execution time: {time.time() - start_time:.2f}s")
